@ONLINE{iplab,
title = {IPLab, 2015.},
howpublished = {\url{http://iplab.dmi.unict.it/}}
}
@inproceedings{goletto2024amego,
    title={AMEGO: Active Memory from long EGOcentric videos},
    author={Goletto, Gabriele and Nagarajan, Tushar and Averta, Giuseppe and Damen, Dima},
    booktitle={European Conference on Computer Vision},
    year={2024}
}
@ARTICLE{Damen2021PAMI,
   title={The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines},
   author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
   year={2021},
   volume={43},
   number={11},
   pages={4125-4141},
   doi={10.1109/TPAMI.2020.2991965}
} 
@article{ragusa2023enigma51,
    title={ENIGMA-51: Towards a Fine-Grained Understanding of Human-Object Interactions in Industrial Scenarios}, 
    author={Francesco Ragusa and Rosario Leonardi and Michele Mazzamuto and Claudia Bonanno and Rosario Scavo and Antonino Furnari and Giovanni Maria Farinella},
    journal   = {IEEE Winter Conference on Application of Computer Vision (WACV)},
    year      = {2024}
}
@article{NUNEZMARCOS2022175,
title = {Egocentric Vision-based Action Recognition: A survey},
journal = {Neurocomputing},
volume = {472},
pages = {175-197},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.11.081},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221017586},
author = {Adrián Núñez-Marcos and Gorka Azkune and Ignacio Arganda-Carreras},
keywords = {Deep learning, Computer vision, Human action recognition, Egocentric vision, Few-shot learning},
abstract = {The egocentric action recognition EAR field has recently increased its popularity due to the affordable and lightweight wearable cameras available nowadays such as GoPro and similars. Therefore, the amount of egocentric data generated has increased, triggering the interest in the understanding of egocentric videos. More specifically, the recognition of actions in egocentric videos has gained popularity due to the challenge that it poses: the wild movement of the camera and the lack of context make it hard to recognise actions with a performance similar to that of third-person vision solutions. This has ignited the research interest on the field and, nowadays, many public datasets and competitions can be found in both the machine learning and the computer vision communities. In this survey, we aim to analyse the literature on egocentric vision methods and algorithms. For that, we propose a taxonomy to divide the literature into various categories with subcategories, contributing a more fine-grained classification of the available methods. We also provide a review of the zero-shot approaches used by the EAR community, a methodology that could help to transfer EAR algorithms to real-world applications. Finally, we summarise the datasets used by researchers in the literature.}
}
@misc{ma2024drvideodocumentretrievalbased,
      title={DrVideo: Document Retrieval Based Long Video Understanding}, 
      author={Ziyu Ma and Chenhui Gou and Hengcan Shi and Bin Sun and Shutao Li and Hamid Rezatofighi and Jianfei Cai},
      year={2024},
      eprint={2406.12846},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.12846}, 
}


@misc{park2025framesusefulefficientstrategies,
      title={Too Many Frames, Not All Useful: Efficient Strategies for Long-Form Video QA}, 
      author={Jongwoo Park and Kanchana Ranasinghe and Kumara Kahatapitiya and Wonjeong Ryu and Donghyun Kim and Michael S. Ryoo},
      year={2025},
      eprint={2406.09396},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09396}, 
}

@misc{wang2024videoagentlongformvideounderstanding,
      title={VideoAgent: Long-form Video Understanding with Large Language Model as Agent}, 
      author={Xiaohan Wang and Yuhui Zhang and Orr Zohar and Serena Yeung-Levy},
      year={2024},
      eprint={2403.10517},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.10517}, 
}

@misc{wang2024lifelongmemoryleveragingllmsanswering,
      title={LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos}, 
      author={Ying Wang and Yanlai Yang and Mengye Ren},
      year={2024},
      eprint={2312.05269},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.05269}, 
}

@misc{wang2025videotreeadaptivetreebasedvideo,
      title={VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos}, 
      author={Ziyang Wang and Shoubin Yu and Elias Stengel-Eskin and Jaehong Yoon and Feng Cheng and Gedas Bertasius and Mohit Bansal},
      year={2025},
      eprint={2405.19209},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.19209}, 
}

@misc{wu2022memvitmemoryaugmentedmultiscalevision,
      title={MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition}, 
      author={Chao-Yuan Wu and Yanghao Li and Karttikeya Mangalam and Haoqi Fan and Bo Xiong and Jitendra Malik and Christoph Feichtenhofer},
      year={2022},
      eprint={2201.08383},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.08383}, 
}

@misc{li2024llmsmeetlongvideo,
      title={LLMs Meet Long Video: Advancing Long Video Question Answering with An Interactive Visual Adapter in LLMs}, 
      author={Yunxin Li and Xinyu Chen and Baotain Hu and Min Zhang},
      year={2024},
      eprint={2402.13546},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13546}, 
}

@misc{qian2024streaminglongvideounderstanding,
      title={Streaming Long Video Understanding with Large Language Models}, 
      author={Rui Qian and Xiaoyi Dong and Pan Zhang and Yuhang Zang and Shuangrui Ding and Dahua Lin and Jiaqi Wang},
      year={2024},
      eprint={2405.16009},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.16009}, 
}

@misc{ren2024timechattimesensitivemultimodallarge,
      title={TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding}, 
      author={Shuhuai Ren and Linli Yao and Shicheng Li and Xu Sun and Lu Hou},
      year={2024},
      eprint={2312.02051},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.02051}, 
}

@misc{song2024moviechatdensetokensparse,
      title={MovieChat: From Dense Token to Sparse Memory for Long Video Understanding}, 
      author={Enxin Song and Wenhao Chai and Guanhong Wang and Yucheng Zhang and Haoyang Zhou and Feiyang Wu and Haozhe Chi and Xun Guo and Tian Ye and Yanting Zhang and Yan Lu and Jenq-Neng Hwang and Gaoang Wang},
      year={2024},
      eprint={2307.16449},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.16449}, 
}

@misc{mangalam2023egoschemadiagnosticbenchmarklongform,
      title={EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding}, 
      author={Karttikeya Mangalam and Raiymbek Akshulakov and Jitendra Malik},
      year={2023},
      eprint={2308.09126},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.09126}, 
}

@misc{arnab2021unifiedgraphstructuredmodels,
      title={Unified Graph Structured Models for Video Understanding}, 
      author={Anurag Arnab and Chen Sun and Cordelia Schmid},
      year={2021},
      eprint={2103.15662},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.15662}, 
}
@misc{baradel2018objectlevelvisualreasoning,
      title={Object Level Visual Reasoning in Videos}, 
      author={Fabien Baradel and Natalia Neverova and Christian Wolf and Julien Mille and Greg Mori},
      year={2018},
      eprint={1806.06157},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1806.06157}, 
}

@misc{cong2021spatialtemporaltransformerdynamicscene,
      title={Spatial-Temporal Transformer for Dynamic Scene Graph Generation}, 
      author={Yuren Cong and Wentong Liao and Hanno Ackermann and Bodo Rosenhahn and Michael Ying Yang},
      year={2021},
      eprint={2107.12309},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2107.12309}, 
}

@misc{jain2016structuralrnndeeplearningspatiotemporal,
      title={Structural-RNN: Deep Learning on Spatio-Temporal Graphs}, 
      author={Ashesh Jain and Amir R. Zamir and Silvio Savarese and Ashutosh Saxena},
      year={2016},
      eprint={1511.05298},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1511.05298}, 
}

@misc{ji2019actiongenomeactionscomposition,
      title={Action Genome: Actions as Composition of Spatio-temporal Scene Graphs}, 
      author={Jingwei Ji and Ranjay Krishna and Li Fei-Fei and Juan Carlos Niebles},
      year={2019},
      eprint={1912.06992},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1912.06992}, 
}

@misc{ma2018attendinteracthigherorderobject,
      title={Attend and Interact: Higher-Order Object Interactions for Video Understanding}, 
      author={Chih-Yao Ma and Asim Kadav and Iain Melvin and Zsolt Kira and Ghassan AlRegib and Hans Peter Graf},
      year={2018},
      eprint={1711.06330},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1711.06330}, 
}

@misc{sun2018actorcentricrelationnetwork,
      title={Actor-Centric Relation Network}, 
      author={Chen Sun and Abhinav Shrivastava and Carl Vondrick and Kevin Murphy and Rahul Sukthankar and Cordelia Schmid},
      year={2018},
      eprint={1807.10982},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1807.10982}, 
}

@misc{wang2018videosspacetimeregiongraphs,
      title={Videos as Space-Time Region Graphs}, 
      author={Xiaolong Wang and Abhinav Gupta},
      year={2018},
      eprint={1806.01810},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1806.01810}, 
}

@misc{price2022unweavenetunweavingactivitystories,
      title={UnweaveNet: Unweaving Activity Stories}, 
      author={Will Price and Carl Vondrick and Dima Damen},
      year={2022},
      eprint={2112.10194},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10194}, 
}
@misc{rodin2023actionscenegraphslongform,
      title={Action Scene Graphs for Long-Form Understanding of Egocentric Videos}, 
      author={Ivan Rodin and Antonino Furnari and Kyle Min and Subarna Tripathi and Giovanni Maria Farinella},
      year={2023},
      eprint={2312.03391},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.03391}, 
}

@InProceedings{Yang_2023_CVPR,
    author    = {Yang, Xitong and Chu, Fu-Jen and Feiszli, Matt and Goyal, Raghav and Torresani, Lorenzo and Tran, Du},
    title     = {Relational Space-Time Query in Long-Form Videos},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {6398-6408}
}

@inproceedings{inproceedings0,
author = {Lee, Yong Jae and Ghosh, J. and Grauman, K.},
year = {2012},
month = {06},
pages = {1346-1353},
title = {Discovering important people and objects for egocentric video summarization},
isbn = {978-1-4673-1226-4},
journal = {Proceedings / CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247820}
}

@InProceedings{Lu_2013_CVPR,
author = {Lu, Zheng and Grauman, Kristen},
title = {Story-Driven Summarization for Egocentric Video},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2013}
} 

@inproceedings{inproceedings1,
author = {Xiong, Bo and Grauman, Kristen},
year = {2014},
month = {09},
pages = {282-298},
title = {Detecting Snap Points in Egocentric Video with a Web Photo Prior},
isbn = {978-3-319-10601-4},
doi = {10.1007/978-3-319-10602-1_19}
}

@InProceedings{Lin_2015_ICCV_Workshops,
author = {Lin, Yen-Liang and Morariu, Vlad I. and Hsu, Winston},
title = {Summarizing While Recording: Context-Based Highlight Detection for Egocentric Videos},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops},
month = {December},
year = {2015}
} 

 @InProceedings{Zhao_2014_CVPR,
author = {Zhao, Bin and Xing, Eric P.},
title = {Quasi Real-Time Summarization for Consumer Videos},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2014}
} 

 @InProceedings{Xiong_2015_ICCV,
author = {Xiong, Bo and Kim, Gunhee and Sigal, Leonid},
title = {Storyline Representation of Egocentric Videos With an Applications to Story-Based Search},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
}

@misc{shan2020understandinghumanhandscontact,
      title={Understanding Human Hands in Contact at Internet Scale}, 
      author={Dandan Shan and Jiaqi Geng and Michelle Shu and David F. Fouhey},
      year={2020},
      eprint={2006.06669},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2006.06669}, 
}

@misc{tang2023egotrackslongtermegocentricvisual,
      title={EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset}, 
      author={Hao Tang and Kevin Liang and Matt Feiszli and Weiyao Wang},
      year={2023},
      eprint={2301.03213},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2301.03213}, 
}

@misc{oquab2024dinov2learningrobustvisual,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2024},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.07193}, 
}

@misc{zhao2022learningvideorepresentationslarge,
      title={Learning Video Representations from Large Language Models}, 
      author={Yue Zhao and Ishan Misra and Philipp Krähenbühl and Rohit Girdhar},
      year={2022},
      eprint={2212.04501},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2212.04501}, 
}

@misc{zhang2024simplellmframeworklongrange,
      title={A Simple LLM Framework for Long-Range Video Question-Answering}, 
      author={Ce Zhang and Taixi Lu and Md Mohaiminul Islam and Ziyang Wang and Shoubin Yu and Mohit Bansal and Gedas Bertasius},
      year={2024},
      eprint={2312.17235},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.17235}, 
}

@misc{li2023blip2bootstrappinglanguageimagepretraining,
      title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models}, 
      author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
      year={2023},
      eprint={2301.12597},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2301.12597}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{singh2022revisitingweaklysupervisedpretraining,
      title={Revisiting Weakly Supervised Pre-Training of Visual Perception Models}, 
      author={Mannat Singh and Laura Gustafson and Aaron Adcock and Vinicius de Freitas Reis and Bugra Gedik and Raj Prateek Kosaraju and Dhruv Mahajan and Ross Girshick and Piotr Dollár and Laurens van der Maaten},
      year={2022},
      eprint={2201.08371},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2201.08371}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}

